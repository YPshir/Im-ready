# datamining-project  
## introduction

This project was written as part of the course "Data Mining" during my degree.  
the full project is in the 
'[project.ipynb](https://github.com/YPshir/datamining-project/blob/master/project.ipynb)' file.  
you can see an introduction in the 
'[read me.pdf](https://github.com/YPshir/datamining-project/blob/master/read%20me.pdf)' file.  

![2](https://user-images.githubusercontent.com/46241467/79245311-9e00fd00-7e80-11ea-9ff8-afd9e7670377.png)  
  
  
  ## Implementation  
  For this project in the "Data Mining" course - I chose a topic and database for which we will perform the process.
  ###  Data Mining  
  Data mining is the practice of automatically searching large stores of data to discover patterns and trends that go beyond simple       analysis. Data mining uses sophisticated mathematical algorithms to segment the data and evaluate the probability of future events.     Data mining is also known as Knowledge Discovery in Data (KDD).
  ##### The key properties of data mining are:
  * Automatic discovery of patterns  
  * Prediction of likely outcomes  
  * Creation of actionable information  
  * Focus on large data sets and databases  
  * Data mining can answer questions that cannot be addressed through simple query and reporting techniques.
  
  ### Main Tools  
  I wrote this project in the Python programming language in the Jupyter Notebook environment, which contains many directories for   realizing and executing data mining processes.   
  
  ### *[Pandas](https://pandas.pydata.org/)*      
  Pandas is an open-source Python package that provides high-performance, easy-to-use data structures and data analysis tools for the   labeled data in Python programming language. Pandas stand for Python Data Analysis Library.   
  Pandas is a perfect tool for data wrangling or munging. It is designed for quick and easy data manipulation, reading, aggregation, and visualization.
Pandas take data in a CSV or TSV file or a SQL database and create a Python object with rows and columns called a data frame. The data frame is very similar to a table in statistical software, say Excel or SPSS.  
###### I used Pandas in the processes:  
  1. Indexing, manipulating, renaming, sorting, merging data frame  
  2. Update, Add, Delete columns from a data frame  
  3. Impute missing files, handle missing data or NANs  
  4. Plot data with histogram or box plot  
  
  ### *[NumPy](https://numpy.org/)*      
One of the most fundamental packages in Python, NumPy is a general-purpose array-processing package. It provides high-performance multidimensional array objects and tools to work with the arrays. NumPy is an efficient container of generic multi-dimensional data.
NumPy’s main object is the homogeneous multidimensional array. It is a table of elements or numbers of the same datatype, indexed by a tuple of positive integers. In NumPy, dimensions are called axes and the number of axes is called rank. NumPy’s array class is called ndarray aka array.  
###### I used NumPy in the processes:  
  1. Basic array operations: add, multiply, slice, flatten, reshape, index arrays  
  2. Advanced array operations: stack arrays, split into sections, broadcast arrays  
  3. Work with DateTime or Linear Algebra  
  4. Basic Slicing and Advanced Indexing in NumPy Python  
  
  ### *[SciPy](https://www.scipy.org/)*      
The SciPy library is one of the core packages that make up the SciPy stack. SciPy builds on the NumPy array object and is part of the stack which includes tools like Matplotlib, Pandas, and SymPy with additional tools,
SciPy library contains modules for efficient mathematical routines as linear algebra, interpolation, optimization, integration, and statistics. The main functionality of the SciPy library is built upon NumPy and its arrays. SciPy makes significant use of NumPy.  
###### I used SciPy in the processes:  
 scipy.cluster.hierarchy for the hierarchy module provides functions for hierarchical and agglomerative clustering. Its features include generating hierarchical clusters from distance matrices, calculating statistics on clusters, cutting linkages to generate flat clusters, and visualizing clusters with dendrograms.  
 
   ### *[Matplotlib](https://matplotlib.org/)*      
Matplotlib is the plotting library for Python that provides an object-oriented API for embedding plots into applications. It is a close resemblance to MATLAB embedded in Python programming language.  
You can create stories with the data visualized with Matplotlib.
With a bit of effort and tint of visualization capabilities, with Matplotlib, you can create just any visualizations.  
###### I used Matplotlib in the processes:  
Histogram, bar charts, scatter plots, area plot to pie plot, Scatter plots, pie charts.  

  ### *[Seaborn](https://seaborn.pydata.org/)*      
data visualization library based on Matplotlib that provides a high-level interface for drawing attractive and informative statistical graphics. seaborn is an extension of Matplotlib with advanced features.
Matplotlib is used for basic plotting; bars, pies, lines, scatter plots and stuff whereas, seaborn provides a variety of visualization patterns with less complex and fewer syntax.  
###### I used Seaborn in the processes:  
  1. Determine relationships between multiple variables (correlation)  
  2. Analyze uni-variate or bi-variate distributions and compare them between different data subsets  
  3. Plot linear regression models for dependent variables  
  4. Provide high-level abstractions, multi-plot grids  
  
  ### *[Scikit Learn](https://scikit-learn.org/stable/)*      
Scikit Learn is a robust machine learning library for Python. It features ML algorithms like SVMs, random forests, k-means clustering, spectral clustering, mean shift, cross-validation and more. NumPy, SciPy and related scientific operations are supported by Scikit Learn with Scikit Learn being a part of the SciPy Stack.
Scikit-learn provides a range of supervised and unsupervised learning algorithms via a consistent interface in Python. Supervised learning models like Naive Bayes to grouping unlabeled data such as KMeans, Scikit learn would be your go-to.  
###### I used Scikit Learn in the processes:  
  1. Classification: Spam detection, image recognition  
  2. Clustering: Drug response, Stock price  
  3. Regression: Customer segmentation, Grouping experiment outcomes  
  4. Dimensionality reduction: Visualization, Increased efficiency  
  5. Model selection: Improved accuracy via parameter tuning  
  6. Pre-processing: Preparing input data as a text for processing with machine learning algorithms.  
  
  
  ### *[Statsmodels](https://www.statsmodels.org/stable/index.html)*      
Scikit Learn is a robust machine learning library for Python. It features ML algorithms like SVMs, random forests, k-means clustering, spectral clustering, mean shift, cross-validation and more. NumPy, SciPy and related scientific operations are supported by Scikit Learn with Scikit Learn being a part of the SciPy Stack.
Scikit-learn provides a range of supervised and unsupervised learning algorithms via a consistent interface in Python. Supervised learning models like Naive Bayes to grouping unlabeled data such as KMeans, Scikit learn would be your go-to.  
###### I used Statsmodels Learn in the processes:  
  1. Classification: Spam detection, image recognition  
  2. Clustering: Drug response, Stock price  
  3. Regression: Customer segmentation, Grouping experiment outcomes  
  4. Dimensionality reduction: Visualization, Increased efficiency  
  5. Model selection: Improved accuracy via parameter tuning  
  6. Pre-processing: Preparing input data as a text for processing with machine learning algorithms.  
  

 
 
  
  
  
  
  
  
